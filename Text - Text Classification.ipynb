{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "### Explore Text Classification using nltk, scikit-learn, and gensim\n",
    "\n",
    "Use a newsgroups dataset: https://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset (more than 18000 newsgroup posts, across 20 news categories)\n",
    "\n",
    "#### Goal: Build ML models that predict the category of a newsgroup post, based on the text of the post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import nltk\n",
    "import gensim \n",
    "from sklearn.datasets import fetch_20newsgroups \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data and take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.politics.guns','rec.sport.baseball'] # Focus on 2 news categories\n",
    "def get_data():\n",
    "    data = fetch_20newsgroups(subset='all',\n",
    "                              shuffle=True,\n",
    "                              categories=categories,\n",
    "                              remove=('headers', 'footers', 'quotes'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.sport.baseball', 'talk.politics.guns']\n",
      "Sample document: For those who didn't figure it out, the below message was a reply to another\n",
      "in sci.crypt, for which the poster put t.p.g. in the Followup-To line. I\n",
      "didn't notice that. Apologies to those who were confused.\n",
      "\n",
      "The substance makes little sense unless one reads the prior messages.\n",
      "\n",
      "However, I don't wish to enter into this discussion here, as it will be yet\n",
      "another rehearsal of a long-tired set of arguments. Suffice it to say that I\n",
      "disagree both with the interpretation of \"well-regulated\" in the Second\n",
      "Amendment offered by gun lovers, and what I think to be their distortion of\n",
      "the same phrase in the associated Federalist papers. My Webster and my\n",
      "reading of the language convinces me that the word meant both under control,\n",
      "and disciplined, and not 'of good marksmanship'. I think the latter a\n",
      "special interest pleading. No one has yet shown a contemporateous reference\n",
      "in which \"well regulated\" unambiguously meant 'of good marksmanship', and\n",
      "not under control/disciplined, etc.\n",
      "\n",
      "Thus I continue to believe the Second Amendment is a militia clause and not\n",
      "an 'arming everyone' clause. Others are welcome to disagree (as I know many\n",
      "do) and little would be served by rehashing this topic in this particular\n",
      "forum.\n",
      "\n",
      "To avoid flames, or unproductive rehashings, I note that I've come in here\n",
      "to post this one message, just to clarify the one below. I'm now outta here\n",
      "again though I'm available via e-mail.\n",
      "\n",
      "David\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Class label: 1\n",
      "Actual class label: talk.politics.guns\n"
     ]
    }
   ],
   "source": [
    "# get text data and their labels\n",
    "dataset = get_data()\n",
    "print(dataset.target_names)\n",
    "\n",
    "corpus, labels = dataset.data, dataset.target\n",
    "\n",
    "print('Sample document:', corpus[10])\n",
    "print('Class label:',labels[10])\n",
    "print('Actual class label:', dataset.target_names[labels[10]])\n",
    "\n",
    "# split training dataset and testing dataset\n",
    "train_corpus, test_corpus, train_labels, test_labels = train_test_split(corpus,\n",
    "                                                                        labels,\n",
    "                                                                        test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepape features for ML \n",
    "### {bow, tfidf, word2vec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bow features\n",
    "from sklearn.feature_extraction.text import CountVectorizer #tokenizes and counts words\n",
    "\n",
    "# build bag of words features' vectorizer and get features\n",
    "bow_vectorizer=CountVectorizer(min_df=1, ngram_range=(1,1))\n",
    "bow_train_features = bow_vectorizer.fit_transform(train_corpus)\n",
    "bow_test_features = bow_vectorizer.transform(test_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #alternatively, use TfidfTransformer()\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(min_df=1, \n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=(1,1))\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(train_corpus)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(test_corpus)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize documents for word2vec\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in train_corpus]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in test_corpus]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word2vec model                   \n",
    "wv_model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               size=200,                          #set the size or dimension for the word vectors \n",
    "                               window=60,                        #specify the length of the window of words taken as context\n",
    "                               min_count=10)                   #ignores all words with total frequency lower than                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector \n",
    "   \n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# averaged word vector features from word2vec\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=wv_model,\n",
    "                                                 num_features=200)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=wv_model,\n",
    "                                                num_features=200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define metrics for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# define a function to evaluate our classification models based on four metrics\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \n",
    "    print ('Accuracy:', np.round(                                                    \n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "    print ('Precision:', np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "    print ('Recall:', np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "    print ('F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how to train and evaluate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that trains the model, performs predictions and evaluates the predictions\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evaluate model prediction performance   \n",
    "    get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate {mnb, svm} with bow features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "Precision: 0.95\n",
      "Recall: 0.95\n",
      "F1 Score: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100)\n",
    "\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n",
      "Precision: 0.92\n",
      "Recall: 0.89\n",
      "F1 Score: 0.91\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine with bag of words features\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate {svm} with {tfidf, word2vec} features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n",
      "Precision: 0.95\n",
      "Recall: 0.9\n",
      "F1 Score: 0.93\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with tfidf features\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "Precision: 0.73\n",
      "Recall: 0.91\n",
      "F1 Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with averaged word vector features\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFUSION MATRIX (for SVM TFIDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>292</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  292   12\n",
       "1   26  242"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build confusion matrix for SVM TF-IDF-based model\n",
    "cm = metrics.confusion_matrix(test_labels, svm_tfidf_predictions)\n",
    "pd.DataFrame(cm, index=range(0,2), columns=range(0,2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.baseball -> talk.politics.guns\n"
     ]
    }
   ],
   "source": [
    "# Observe false positive output\n",
    "class_names = dataset.target_names\n",
    "print (class_names[0], '->', class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      " Nah, let's reserve rec.sports.idiots for people who POST obvious flamebait, like yourself.  If someone posts something as controversial (not to mention idiotic) as what Austin posted in a widely accessed newsgroup, someone should challenge the statement.  There is a school of thought that suggests that silence = consent.  Whereas this idea may not apply to everything in life, it certainly SHOULD apply to a forum of public discussion, which r.s.b. is.  If you've been reading r.s.b. lately, you'll find that even elementary school children have had access to our postings, alibet in an edited form.  It's making me think a little more carefully about some of the things I post.  In conclusion, if someone like Austin wants to post his drivel in some obscure newsgroup that I don't read, fine.  He's got the right to rant, rave, and drool all he wants to in the name of free speech.  But if he drools in a newsgroup that I read, then I will support the right of anyone to provide rebuttal to his drooling.  Now, of course, you don't have to read any of this. And if you want to cut down on flames, then     DON'T POST FLAMEBAIT! (You don't have to respond to flames, either.  Saves cyberspace)  \n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      " It is ironic that in any post that criticizes langauge ability, the critic invariably makes a mistake himself (\"english\" is generally written \"English\".)   Oddly, I do not see that I have contested any of that. Perhaps you, with  assuredly greater \"english\" ability can explain, in tiny words that I might grasp their meaning, precisely WHERE I infer that you have said any of those things?    No Mr Fisher, you should place the burden of proof on the one who makes the allegation in the first place. You do not. Perhaps you might explain why that is? As for the email route, Mr Fisher, you might have tried that yourself. \n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "         When I say \"black,\" I mean US-born black people for the purposes of this discussion.  Hispanic players were in baseball before 1947, and one  team in the 50's signed lots of hispanics because they went over better with the local audience than blacks did.     Don't know.  But remember: this is the country that had special racial laws for one group and one group only: blacks.  Our national history  includes huge, long-term, global tensions regarding the black minority;  the hispanic minority, while often discriminated against, has never been the object of national obsession.   Absolutely.  As I said before, I expect that this effect is disappearing. But it certainly did exist, and all out talk of TWG's and all that is  not without some small reason.   Well, there's the list.  Go for it!  I'll cull some more names as I go. I expect you're right, btw.\n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "Ted Frank's list of underpaid players was this:  What do all of these players have in common?  They do not qualify for  arbitration.  They were never free agents.  It's called the reserve clause.  Look it up.  And a year from now we will whine about how several of these guys are way  overpaid and getting outrageous raises in arb.  Humbug.\n"
     ]
    }
   ],
   "source": [
    "# Look at some misclassified documents in detail\n",
    "import re\n",
    "\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 0 and predicted_label == 1:\n",
    "        print ('Actual Label:', class_names[label])\n",
    "        print ('Predicted Label:', class_names[predicted_label])\n",
    "        print ('Document:-')\n",
    "        print (re.sub('\\n', ' ', document))\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## New Machine Mearning methods and evaluation \n",
    "### For assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "Precision: 0.87\n",
      "Recall: 0.79\n",
      "F1 Score: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#Decision tree with bag of words features\n",
    "DT_bow_predictions = train_predict_evaluate_model(classifier=DecisionTreeClassifier(),\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "Precision: 0.79\n",
      "Recall: 0.79\n",
      "F1 Score: 0.79\n"
     ]
    }
   ],
   "source": [
    "#Decision tree with TFIDF features\n",
    "DT_tfidf_predictions = train_predict_evaluate_model(classifier=DecisionTreeClassifier(),\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "Precision: 0.8\n",
      "Recall: 0.75\n",
      "F1 Score: 0.77\n"
     ]
    }
   ],
   "source": [
    "#Decision tree with averaged word vector features\n",
    "DT_avgwv_predictions = train_predict_evaluate_model(classifier=DecisionTreeClassifier(),\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>272</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  272   32\n",
       "1   56  212"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build confusion matrix for SVM TF-IDF-based model\n",
    "cm1 = metrics.confusion_matrix(test_labels, DT_bow_predictions)\n",
    "pd.DataFrame(cm1, index=range(0,2), columns=range(0,2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##How to improve?\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "Voting = VotingClassifier(estimators=[('mnb',mnb),('svm',svm)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "Precision: 0.97\n",
      "Recall: 0.86\n",
      "F1 Score: 0.91\n"
     ]
    }
   ],
   "source": [
    "#Voting with bag of words features\n",
    "voting_bow_predictions = train_predict_evaluate_model(classifier=Voting,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
